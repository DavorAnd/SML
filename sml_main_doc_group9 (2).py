# -*- coding: utf-8 -*-
"""SML Main doc Group9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hi48jlf2lYDvdACr6uVp2SfHoN4Kkf-3

#Assignment introduction

**Time to predict some stuff!** <br> For this assignment, we will try to predict house prices based on the features of the house in question. This means we need to do the following:

1. Load in the data (as per usual)
2. Perform some EDA to get a better understanding of the data
3. Clean up the data
4. Perform feature engineering and choose our feature dimensions
5. Create the feature and target matrix (our X's and y's)
6. Create, fit a model and evaluate performance
7. Set up a data prediction pipeline

This assignment was very much made to give you a lot of creative freedom in how you want to approach your data engineering and model creation. Creating good SML models takes a lot of thinking, effort and testing. You can expect to go back and forth in the notebook a lot to change earlier data engineering steps, so keep your assigned variable names consistent!<br><br>

(**Note:** There is no shame in not creating a super well-performing model, as long as you try out the different methods involved in SML. As the picture below illustrates, sometimes it just goes wrong) <br><br>

The order in which you perform the different steps is more or less up to you, as long as you end up with some sort of trained model that is suitable for this type of prediction. <br>
The way this notebook is laid out, is just to give you a general direction guide in terms of the overarching concepts; data filtering, data engineering, model fitting, testing and evaluation as well as setting up a data pipeline. If it's easier, you're welcome to create a separate notebook instead of working in this one, as long as you cover the tasks included.

![](https://aaubs.github.io/ds-master/media/ML_Daddy.png)

This is not an exhaustive list, but for this assignment you **will** need the following:
1. From Scikitlearn:
- Some sort of encoding library of your own choice
- train_test_split (unless you want to do it manually for whichever reason)
- Some sort of data scaler
2. A visualization library:
- I recommend matplotlib.pyplot and seaborn, but you can try to use altair if you want to
3. Pandas (not the bamboo eating kind)
- Because, duh, we need them dataframes!
4. Joblib or Pickle
- To save each component of the entire process

# Importing modules and loading in the data

**Link to the Data:** <br> https://www.kaggle.com/datasets/harishkumardatalab/housing-price-prediction?resource=download
"""

!pip install xgboost -U -q
!pip install sklearn -U -q

import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt #plots
import gradio as gr

df = pd.read_csv("/Housing.csv") #read in the data file with using the panda libary

df.head() # the first 5 rows of the data

df.isnull().count()

"""#EDA

**First things first** <br> It's always a good idea to start out with checking for missing values, and dealing with them if there are any. <br> Do we have any missing values that we need to take care of? If so, find a way to handle the missing data (ie removing or replacing/filling them)
"""

print(df.info()) # here we are checking the data, and we can see that there is non null, which could mean that we do not need to clean the dataset from null values

print(df.isnull().sum())

"""**Lets visualize some of the dimensions to get a better idea of our data. <br> Create a plot that shows the distribution of the price dimension. What can we see?**"""

df.columns

numerical_columns = ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'furnishingstatus']

for column in numerical_columns:
    plt.hist(df[column], bins=30)
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

    # Here we have visualized the distribution of price compared in every columns. So we can se which columns that is inbalanced.
    # here we can see that we have to created some dummy values to use instead of yes and no.
    # And we have some outlines in our data set because of the prices

"""**Correlations impact our predictions through statistical inference. Thus understanding which correlations we may be dealing with, is a good tool for choosing our dimensions for the feature matrix. Often these can be quite logical with some understanding of what the data represents <br> Check the correlations of the dimensions**"""

House_features = ['price','area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'furnishingstatus']
correlation = df[House_features].corr()

plt.figure(figsize=(10,8))
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title('Correlation Between Home Features')
plt.show()



"""#Data preprocessing

##Data cleaning

**Do we have any outliers in our data that may affect our prediction? If so, remove them if you think they could cause issues**
"""

#Replace the yes and no value with true and false

df.columns

df = df.drop('hotwaterheating', axis=1) # We are dropping the hotwaterheating as the histogram is showing imbalance relations between yes and no.

mapping = {'no':False,'yes':True}
df.replace({"mainroad"  :mapping}, inplace=True)

mapping = {'no':False,'yes':True}
df.replace({"guestroom"  :mapping}, inplace=True)

mapping = {'no':False,'yes':True}
df.replace({"basement" :mapping}, inplace=True)

mapping = {'no':False,'yes':True}
df.replace({"airconditioning" :mapping}, inplace=True)

mapping = {'no':False,'yes':True}
df.replace({"prefarea"  :mapping}, inplace=True)

df.head()

"""**Is there any other cleaning that needs to be done? If you believe so, then perform the remaining cleaning and then proceed to the next step**"""

len(df.price) # as it looks liked we would have some outliers from our histrogrom i choose the remove outlies from the data with the z values

df['price_z'] = (df['price'] - df['price'].mean())/df['price'].std(ddof=0)
df['price_z'] = df['price_z'].abs()
df = df[df.price_z < 2]

len(df.price) # We have removed some of the outliers which is also shown pÃ¥ the len

df['area_z'] = (df['area'] - df['area'].mean())/df['area'].std(ddof=0)
df['area_z'] = df['area_z'].abs()
df = df[df.area_z < 2] # We are doing the same for the outliers in area. same as the price

#Filtering the bedrooms where we drop the 5 and 6 bedroom houses
df = df[df.bedrooms.isin([1, 2, 3, 4])]

#Checking the uniques for bedrooms
df.bedrooms.unique()

#Filtering the bathrooms where we only include 1 and 2 bathrooms
df = df[df.bathrooms.isin([1, 2])]

#Checking the uniques for bathrooms
df.bathrooms.unique()

#Filtering parking where we only include 1 and 2 parking
df = df[df.parking.isin([1, 2])]

#Checking the uniques for parking
df.parking.unique()

#Filtered data
selected_df = df[['price_z','area_z', 'bedrooms', 'bathrooms', 'stories', 'parking','mainroad', 'guestroom', 'basement', 'airconditioning', 'prefarea', 'furnishingstatus']]

"""##Feature engineering

**So far we've gotten an idea of how our data is correlated, and we may already have an idea of which features we wish to use for our feature matrix. We can however do more than simply check correlations between data points. We can check what's known as feature importances. In order to do this we need to engineer the data in a way so that we can feed it to the SML model we choose.**

**BONUS TASK:** <br>
It may be possible to define new features, for example as a combination of two existing ones in order to increase predictability. If you believe you can create new features, this is the time to do it <br>
(Note: This step is not necessarily needed, but more if you're feeling adventurous, or if your model is not performing as well as you had hoped)
"""



"""**In order to fit a model to our data, we need to encode the categorical values for prediction. <br> Encode the categorical features in the dataframe**"""

X = selected_df.iloc[:,1:] #We are choosing to have these coulmns as ours features

X

y = selected_df.price_z # y will in this data set be the price for the homes

X

y

from sklearn.preprocessing import OneHotEncoder
import itertools

ohe_X = OneHotEncoder(sparse=False) #Here we are making our categorical variables as numerical values. This means that instead of text such as furnished, we would have 3 coulmns with 0 or 1 with the difference possibilities

X_ohe = ohe_X.fit_transform(X.iloc[:,-1:]) #We are making a matrix out of the Ohe_x value

X_ohe

columns_X_ohe = list(itertools.chain(*ohe_X.categories_))

columns_X_ohe

X_cat = pd.DataFrame(X_ohe, columns = columns_X_ohe)

X_cat

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X

transformed_nummerical = scaler.fit_transform(X.iloc[:,0:5]) #Here we are transforming the value so the values are on a similar scale.

transformed_nummerical

X.iloc[:,0:5] = transformed_nummerical

X

X.index = range(len(X))
X_cat.index = range(len(X_cat))

X_enc = X.iloc[:,:-1].join(X_cat) # here we are joining the two tables.

X_enc

"""##Creating target and feature + final evaluation of dimensions

**Separate the target feature from the rest**
"""



"""**Next up:**<br> I want you to evaluate whats known as the feature importances of the data. (Hint: Most SML models has a class attribute for this) <br>
**Reflect:**<br> Do we evaluate feature importances *before* or *after* we do the train_test_split? What are some possible issues/benefits with either approach? <br>
**Task:** Evaluate the feature importances of your data.

**Solution:** I do this *BEFORE* the feature importances, as I wish to increase the models ability to generalize to new data. If we base our selection on feature importances of all the data, it may lead to overly optimistic model evaluation.
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_enc, y, test_size=0.3) # 30% of our data is used as test data

"""**Now that you have selected your target features through whichever method you preferred, you need to remove the excess features that you don't need.** <br>
*Note: If you've already performed the train_test_split, remember to remove them from both the test and training data*
"""



from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

model_xgb = XGBRegressor()

model_xgb.fit(X_train, y_train)



print('Model XGB' + ' ' + str(model_xgb.score(X_train, y_train))) #The score could indicate that we have overfitted the model, since we get such a high score.

y_pred_train = model_xgb.predict(X_train)

mean_squared_error(y_train, y_pred_train, squared=False)



from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.ensemble import RandomForestRegressor

model_ols = LinearRegression()
model_el = ElasticNet()
model_rf = RandomForestRegressor()

model_ols.fit(X_train, y_train)
model_el.fit(X_train, y_train)
model_rf.fit(X_train, y_train)



print('Model OLS' + ' ' + str(model_ols.score(X_test, y_test))) # The score could indicate that we have overfitted the model, since we got a perfect score with the XGB and a low score with the other models
print('Model EL' + ' ' + str(model_el.score(X_test, y_test)))
print('Model RF' + ' ' + str(model_rf.score(X_test, y_test)))









"""#Fitting, testing and evaluating the model

**Okay! Now we got our feature and target matrix sorted (for now), we finally get to actually create our machine learning model.** <br><br>
**Task:** Select a suitable model for the type of prediction we are trying to make, fit it to your data and evaluate the performance using appropriate metrics. <br><br>
**Reflect**: Is the model performing well? If not, how can we increase the performance?
"""









"""##**BONUS TASK - Hyperparameter tuning using GridSearchCV:**

One way to increase model performance is to perform what's known as a grid search of the hyperparameters of the model. Basically we try out different combinations of these hyperparameters in order to find the most optimal setup based on some form of scoring metric. Give it a try!  <br><br>
**NOTE:** These can take a while to run, so whilst it is a good approach, it ***can*** also cost a lot of time, and as you may have experienced, Colab tends to time out after a while. A way to think of it is the following;<br><br>

$$
\text {Total models tested} = \text {(Number of variables for parameter 1)} \space \times \text {(Number of variables for parameter 2)} \space \times \text {(Number of variables for parameter 3)} \space \text {...}\space \times \text {(Number of variables for parameter n)}
$$ <br>

**So if you're testing out the following param_grid;**
<br>3 variations of parameter 1,
<br>4 variations of parameter 2,
<br> 2 variations of parameter 3,
<br> 2 variations of parameter 5,
<br>5 variations of parameter 6
<br><br>**You get the following:** <br><br>

$$
\text {Total models tested} = 3 \times 4 \times 2 \times 2 \times 5 = 240
$$ <br>
As you can see it goes up quick, as we are already testing 240 versions of the model with different parameters. Whilst testing upwards of even 100 variations doesn't necessarily take that long (which I tried), you should still be careful to not just increase it to try out all possible combinations there are
"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# scorer = make_scorer(r2_score)
scorer = make_scorer(mean_squared_error)

parameters_rf = {'bootstrap': [True, False], # We have chosen to go with the RF model, and here we are trying to fine tune the parameters
 'max_depth': [5,10, 15,20],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [5,10,15,20,25, 50]}

# erform grid search on the classifier using 'scorer' as the scoring method.
grid_obj = GridSearchCV(model_rf, parameters_rf, scoring=scorer)

grid_fit = grid_obj.fit(X_enc, y)

# Get the estimator.
best_reg = grid_fit.best_estimator_

# Fit the new model.
best_reg.fit(X_train, y_train)

# Model performance on TRAIN data
# Here we can see a perfect score which indicates that we have a overfitting model
best_reg.score(X_train, y_train)

# Model performance on TEST data
# Here we can see we dont get a high score which indicates that we have a overfitting model, since it does not answer well to new data

best_reg.score(X_test, y_test)

!pip install shap

import shap

explainer = shap.TreeExplainer(best_reg)

shap_values = explainer.shap_values(X_enc)

shap.summary_plot(shap_values, X_enc, plot_type="bar") # We can see that stories and area have the most effect on our model, which makes sense in the bigger the building the bigger the price.

shap.summary_plot(shap_values, X_enc)

shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[1,:], X_enc.iloc[1,:])

"""## **Save the model components**

**When we have created our model, we want to able to use it outside of our development notebook, for this purpose we need to save each component we used for preprocessing, as well as the model itself**<br>
**Task:** <br>
Save the model components (the prediction model itself, the scaler and the label encoder)
"""

import pickle

selected_df.iloc[3,:]

X_test.iloc[3,:]

with open('model_RF.pkl', 'wb') as model_file: #Saving the file with pickle
    pickle.dump(best_reg, model_file)

pickle.dump(scaler, open('scaler.pkl','wb'))

pickle.dump(ohe_X, open('ohe.pkl','wb'))

X.to_json('X.json')
selected_df.to_json('selected_df.json')

pickle.dump(best_reg, open('model_xgb.pkl','wb'))

pickle.dump(shap_values, open('shap_values.pkl','wb'))

"""#Creating the data pipeline

**Finally we want to streamline our preprocessing and prediction methods for new data points. To do this, we create what's known as a data pipeline. It's basically a function that performs the *same* preprocessing steps as what we did earlier on a new observation** <br><br>
**Task 1:** <br> Load in your components, and create a data pipeline function that will perform the preprocessing steps you did earlier all in one. Apply it to a new observation. <br><br>
**Task 2:** <br> Load in your model, and create a prediction function that will predict an outcome based on this new observation
"""

X_enc

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

#Pipeline

pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler()),
    ('model', RandomForestRegressor())
  ])


pipeline.fit(X_train, y_train) # Fit the pipeline on the training data
print("model score: %.3f" % pipeline.score(X_test, y_test))

X_enc.head()

"""You can either create a simple gradio interface, or alternatively create a streamlit application."""